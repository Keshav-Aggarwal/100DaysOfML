{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/root/miniconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/root/miniconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTER_LR = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 10\n",
    "MAX_STEPS = 1000\n",
    "IMAGE_SIZE = 28\n",
    "OUTPUT_NAMES = [\"fc2/Relu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeightsVariable(shape, name = 'weights'):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1, name=name))\n",
    "\n",
    "def BiasVariable(shape, name = 'biases'):\n",
    "    return tf.Variable(tf.constant(0.1, shape=[shape], name='biases'))\n",
    "\n",
    "def Conv2d(x, W, B, stride = 1, padding = 'VALID', activation_fun = True):\n",
    "    filter_size = W.get_shape().as_list()\n",
    "    pad_size = filter_size[0] // 2\n",
    "    pad_mat = np.array([[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]])\n",
    "    \n",
    "    x = tf.pad(x, pad_mat)\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding = padding)\n",
    "    x = tf.nn.bias_add(x, B)\n",
    "    \n",
    "    if(activation_fun == True):\n",
    "        return tf.nn.relu(x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def MaxPool2d(x, k = 2):\n",
    "    return tf.nn.max_pool(x, ksize = [1, k, k, 1], strides = [1, k, k, 1], padding = 'VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(images, channels = 1):\n",
    "    #COONVOLUTION 1\n",
    "    with tf.name_scope('Conv1'):\n",
    "        weights = WeightsVariable([3, 3, channels, 32])\n",
    "        bias = BiasVariable(32)\n",
    "        conv1 = Conv2d(images, weights, bias)\n",
    "        \n",
    "    with tf.name_scope('Conv2_m'):\n",
    "        weights = WeightsVariable([3, 3, 32, 64])\n",
    "        bias = BiasVariable(64)\n",
    "        conv2 = Conv2d(conv1, weights, bias)\n",
    "        pool1 = MaxPool2d(conv2)\n",
    "        \n",
    "    with tf.name_scope('Conv3'):\n",
    "        weights = WeightsVariable([3, 3, 64, 64])\n",
    "        bias = BiasVariable(64)\n",
    "        conv3 = Conv2d(pool1, weights, bias)\n",
    "    \n",
    "    with tf.name_scope('Conv4_m'):\n",
    "        weights = WeightsVariable([3, 3, 64, 128])\n",
    "        bias = BiasVariable(128)\n",
    "        conv4 = Conv2d(conv3, weights, bias)\n",
    "        pool2 = MaxPool2d(conv4)\n",
    "    \n",
    "    with tf.name_scope('flatten'):\n",
    "        flat = tf.layers.flatten(pool2)\n",
    "    \n",
    "    input_flat_shape = pool2\n",
    "    \n",
    "    with tf.name_scope('fc1'):\n",
    "        weights = WeightsVariable([6272, 1024])\n",
    "        biases = BiasVariable(1024)\n",
    "        fc1 = tf.nn.relu(tf.matmul(flat, weights) + biases)\n",
    "\n",
    "    with tf.name_scope('fc2'):\n",
    "        weights = WeightsVariable([1024, 10])\n",
    "        biases = BiasVariable(10)\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases)\n",
    "    print(fc2)    \n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_metrics(logits, labels):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = labels, \n",
    "                                                                   logits = logits, \n",
    "                                                                   name = 'softmax')\n",
    "    return tf.reduce_mean(cross_entropy, name = 'softmax_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the optimizer by taking the loss\n",
    "def training(loss):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    global_step = tf.Variable(0, name = 'global_step', trainable = False)\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(STARTER_LR, \n",
    "                                               global_step = global_step, \n",
    "                                               decay_steps = 1000, \n",
    "                                               decay_rate = 0.7, \n",
    "                                               staircase = True)\n",
    "\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum = 0.9)\n",
    "    train_op = optimizer.minimize(loss, global_step = global_step)\n",
    "    return train_op\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "    correct = tf.nn.in_top_k(logits, labels, k = 1)\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(sess, eval_correct, image_placeholder, labels_placeholder, data_set, summary):\n",
    "    true_count = 0\n",
    "    steps_per_epoch = data_set.num_examples // BATCH_SIZE\n",
    "    num_examples = steps_per_epoch * BATCH_SIZE\n",
    "    \n",
    "    for steps in range(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set, image_placeholder, labels_placeholder)\n",
    "        log, correctness = sess.run([summary, eval_correct], feed_dict = feed_dict)\n",
    "        true_count += correctness\n",
    "    \n",
    "    precision = float(true_count) / num_examples\n",
    "    tf.summary.scalar('Precision', tf.constant(precision))\n",
    "    print('Num examples %d, Num Correct: %d Precisiokn @ 1: %0.04f' %\n",
    "          (num_examples, true_count, precision))\n",
    "    \n",
    "    return log    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size, shape = [28, 28, 1]):\n",
    "    image_placeholder = tf.placeholder(tf.float32, shape = (None, shape[0], shape[1], shape[2]))\n",
    "    label_placeholder = tf.placeholder(tf.int32, shape = (None))\n",
    "    return image_placeholder, label_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_set, image_placeholder, label_placeholder):\n",
    "    images_feed, labels_feed = data_set.next_batch(BATCH_SIZE)\n",
    "    feed_dict = {\n",
    "        image_placeholder: np.reshape(images_feed, (-1, 28, 28, 1)),\n",
    "        label_placeholder: labels_feed\n",
    "                }\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(Dataset):\n",
    "    with tf.Graph().as_default():\n",
    "        images_pl, labels_pl = placeholder_inputs(BATCH_SIZE)\n",
    "        logits = network(images_pl)\n",
    "        loss = loss_metrics(logits = logits, labels = labels_pl)\n",
    "        train_op = training(loss)\n",
    "        eval_correct = evaluation(logits, labels_pl)\n",
    "        \n",
    "        summary = tf.summary.merge_all()\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5) #OPTIONAL\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options = gpu_options))\n",
    "        \n",
    "        summary_writer = tf.summary.FileWriter(\"/tmp/tf/eg/log\", \n",
    "                                               graph = tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(\"tmp/tf/eg/validation/log\", \n",
    "                                            graph = tf.get_default_graph())\n",
    "        \n",
    "        sess.run(init)\n",
    "        for steps in range(MAX_STEPS):\n",
    "            start_time = time.time()\n",
    "            feed_dict = fill_feed_dict(Dataset.train, images_pl, labels_pl)\n",
    "            \n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict = feed_dict)\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            if (steps%100 == 0):\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (steps, loss_value, duration))\n",
    "                summary_str = sess.run(summary, feed_dict = feed_dict)\n",
    "                summary_writer.add_summary(summary_str, steps)\n",
    "                summary_writer.flush()\n",
    "                \n",
    "            if (steps + 1) % 1000 == 0 or (steps + 1) == MAX_STEPS:\n",
    "                checkpoint_file = os.path.join(\"model\", \"model.ckpt\")\n",
    "                saver.save(sess, checkpoint_file, global_step=steps)\n",
    "                print('Validation Data Eval:')\n",
    "                log = do_eval(sess,\n",
    "                              eval_correct,\n",
    "                              images_pl,\n",
    "                              labels_pl,\n",
    "                              Dataset.validation,\n",
    "                              summary)\n",
    "                test_writer.add_summary(log, steps)\n",
    "                \n",
    "        graphdef = tf.get_default_graph().as_graph_def()\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(sess,\n",
    "                                                                    graphdef,\n",
    "                                                                    OUTPUT_NAMES)\n",
    "        return tf.graph_util.remove_training_nodes(frozen_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "MNIST_DATASETS = tf.contrib.learn.datasets.load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc2/Relu:0\", shape=(?, 10), dtype=float32)\n",
      "Step 0: loss = 9.01 (0.015 sec)\n",
      "Step 100: loss = 2.30 (0.002 sec)\n",
      "Step 200: loss = 2.30 (0.003 sec)\n",
      "Step 300: loss = 2.30 (0.002 sec)\n",
      "Step 400: loss = 2.30 (0.002 sec)\n",
      "Step 500: loss = 2.30 (0.002 sec)\n",
      "Step 600: loss = 2.30 (0.003 sec)\n",
      "Step 700: loss = 2.30 (0.002 sec)\n",
      "Step 800: loss = 2.30 (0.002 sec)\n",
      "Step 900: loss = 2.30 (0.002 sec)\n",
      "Validation Data Eval:\n",
      "Num examples 5000, Num Correct: 4998 Precisiokn @ 1: 0.9996\n",
      "INFO:tensorflow:Froze 12 variables.\n",
      "Converted 12 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "tf_model = run_training(MNIST_DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "graph_def must be a GraphDef proto.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m       \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Parameter to MergeFrom() must be instance of same class: expected tensorflow.GraphDef got Graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-329087b3cacf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# The name var will prefix every op/nodes in your graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Since we load everything in a new graph, this is not needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prefix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'graph_def must be a GraphDef proto.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0minput_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: graph_def must be a GraphDef proto."
     ]
    }
   ],
   "source": [
    "graph = tf.GraphDef()\n",
    "graph.ParseFromString(tf_model.SerializeToString())\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "        # The name var will prefix every op/nodes in your graph\n",
    "        # Since we load everything in a new graph, this is not needed\n",
    "    tf.import_graph_def(graph, name=\"prefix\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
