{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow Classification Template.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "oFd0yka_0-Ul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "frMPmKbS4bZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3431b563-ee9b-48bf-b342-d1a7293effb0"
      },
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "tf.set_random_seed(10)\n",
        "np.random.seed(10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.26.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "veAWnxz80-VQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "d59179df-ed17-4a80-ce21-f0e2ca3267e1"
      },
      "cell_type": "code",
      "source": [
        "MNIST_DATASETS = tf.contrib.learn.datasets.load_dataset(\"mnist\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-e0b27bad68fd>:1: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Ynn1r_c0-Us",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "STARTER_LR = 1e-4\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "MAX_STEPS = 3\n",
        "IMAGE_SIZE = 28\n",
        "OUTPUT_NAMES = [\"fc2/Relu\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUeGUaMN0-Uw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def WeightsVariable(shape, name = 'weights'):\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1, name=name))\n",
        "\n",
        "def BiasVariable(shape, name = 'biases'):\n",
        "    return tf.Variable(tf.constant(0.1, shape=[shape], name='biases'))\n",
        "\n",
        "def Conv2d(x, W, B, stride = 1, padding = 'VALID', activation_fun = True):\n",
        "    filter_size = W.get_shape().as_list()\n",
        "    pad_size = filter_size[0] // 2\n",
        "    pad_mat = np.array([[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]])\n",
        "    \n",
        "    x = tf.pad(x, pad_mat)\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding = padding)\n",
        "    x = tf.nn.bias_add(x, B)\n",
        "    \n",
        "    if(activation_fun == True):\n",
        "        return tf.nn.relu(x)\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "def MaxPool2d(x, k = 2):\n",
        "    return tf.nn.max_pool(x, ksize = [1, k, k, 1], strides = [1, k, k, 1], padding = 'VALID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IC-wHEcH0-Uz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def network(images, channels = 1):\n",
        "    num_c = [8, 16, 32, 64, 64, 128]\n",
        "    #COONVOLUTION 1\n",
        "    with tf.name_scope('Conv1'):\n",
        "        weights = WeightsVariable([3, 3, channels, num_c[0]])\n",
        "        bias = BiasVariable(num_c[0])\n",
        "        conv1 = Conv2d(images, weights, bias)\n",
        "        \n",
        "    with tf.name_scope('Conv2_m'):\n",
        "        weights = WeightsVariable([3, 3, num_c[0], num_c[1]])\n",
        "        bias = BiasVariable(num_c[1])\n",
        "        conv2 = Conv2d(conv1, weights, bias)\n",
        "        pool1 = MaxPool2d(conv2)\n",
        "        \n",
        "    with tf.name_scope('Conv3'):\n",
        "        weights = WeightsVariable([3, 3, num_c[1], num_c[2]])\n",
        "        bias = BiasVariable(num_c[2])\n",
        "        conv3 = Conv2d(pool1, weights, bias)\n",
        "    \n",
        "    with tf.name_scope('Conv4_m'):\n",
        "        weights = WeightsVariable([3, 3, num_c[2], num_c[3]])\n",
        "        bias = BiasVariable(num_c[3])\n",
        "        conv4 = Conv2d(conv3, weights, bias)\n",
        "        pool2 = MaxPool2d(conv4)\n",
        "    \n",
        "    with tf.name_scope('flatten'):\n",
        "        flat = tf.layers.flatten(pool2)\n",
        "    \n",
        "    input_flat_shape = np.int32(flat.shape[1])\n",
        "    \n",
        "    with tf.name_scope('fc1'):\n",
        "        weights = WeightsVariable([input_flat_shape, 64])\n",
        "        biases = BiasVariable(64)\n",
        "        fc1 = tf.nn.relu(tf.matmul(flat, weights) + biases)\n",
        "\n",
        "    with tf.name_scope('fc2'):\n",
        "        weights = WeightsVariable([64, 10])\n",
        "        biases = BiasVariable(10)\n",
        "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases)\n",
        "    return fc2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AEWbuvcQ0-U2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_metrics(logits, labels):\n",
        "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = labels, \n",
        "                                                                   logits = logits, \n",
        "                                                                   name = 'softmax')\n",
        "    return tf.reduce_mean(cross_entropy, name = 'softmax_mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SJLUwfZX0-U6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#returns the optimizer by taking the loss\n",
        "def training(loss):\n",
        "    tf.summary.scalar('loss', loss)\n",
        "    global_step = tf.Variable(0, name = 'global_step', trainable = False)\n",
        "    \n",
        "    learning_rate = tf.train.exponential_decay(STARTER_LR, \n",
        "                                               global_step = global_step, \n",
        "                                               decay_steps = 1000, \n",
        "                                               decay_rate = 0.7, \n",
        "                                               staircase = True)\n",
        "\n",
        "    tf.summary.scalar('learning_rate', learning_rate)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "    train_op = optimizer.minimize(loss)\n",
        "    return train_op\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "57j_wcyV0-U9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluation(logits, labels):\n",
        "    correct = tf.nn.in_top_k(logits, labels, k = 1)\n",
        "    \n",
        "#     correct_prediction = tf.equal(tf.round(tf.nn.sigmoid(logits)), tf.cast(tf.round(labels), tf.float32))\n",
        "#     accuracy1 = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "#     all_labels_true = tf.reduce_min(tf.cast(correct_prediction, tf.float32), 1)\n",
        "#     accuracy2 = tf.reduce_mean(all_labels_true)\n",
        "    \n",
        "    return tf.reduce_sum(tf.cast(correct, tf.int32))#, accuracy1, accuracy2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zp-v0RYc0-VB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def do_eval(sess, eval_correct, image_placeholder, labels_placeholder, data_set, summary):\n",
        "    true_count = 0\n",
        "    steps_per_epoch = data_set.num_examples // BATCH_SIZE\n",
        "    \n",
        "    num_examples = steps_per_epoch * BATCH_SIZE\n",
        "    \n",
        "    for steps in range(steps_per_epoch):\n",
        "        feed_dict = fill_feed_dict(data_set, image_placeholder, labels_placeholder)\n",
        "        log, correctness = sess.run([eval_correct], feed_dict = feed_dict)\n",
        "        true_count += correctness\n",
        "    \n",
        "    precision = float(true_count) / num_examples\n",
        "#     tf.summary.scalar('Precision', tf.constant(precision))\n",
        "    print('Num examples %d, Num Correct: %d Precisiokn @ 1: %0.04f' %\n",
        "          (num_examples, true_count, precision))\n",
        "    \n",
        "    return log    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sgIUHWfj0-VG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def placeholder_inputs(batch_size, shape = [28, 28, 1]):\n",
        "    image_placeholder = tf.placeholder(tf.float32, shape = (None, shape[0], shape[1], shape[2]))\n",
        "    label_placeholder = tf.placeholder(tf.int32, shape = (None))\n",
        "    return image_placeholder, label_placeholder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fp2vpUl80-VK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fill_feed_dict(data_set, image_placeholder, label_placeholder):\n",
        "    images_feed, labels_feed = data_set.next_batch(BATCH_SIZE)\n",
        "    feed_dict = {\n",
        "        image_placeholder: np.reshape(images_feed, (-1, 28, 28, 1)),\n",
        "        label_placeholder: labels_feed\n",
        "                }\n",
        "    return feed_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Za5Tlllw0-VN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def run_training(Dataset):\n",
        "    with tf.Graph().as_default():\n",
        "\n",
        "        images_pl, labels_pl = placeholder_inputs(BATCH_SIZE)\n",
        "        logits = network(images_pl)\n",
        "        loss = loss_metrics(logits = logits, labels = labels_pl)\n",
        "        train_op = training(loss)\n",
        "        eval_correct = evaluation(logits, labels_pl)\n",
        "        summary = tf.summary.merge_all()\n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        \n",
        "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9) #OPTIONAL\n",
        "        sess = tf.Session(config=tf.ConfigProto(gpu_options = gpu_options))\n",
        "        \n",
        "        summary_writer = tf.summary.FileWriter(\"/tmp/tf/eg/log\", \n",
        "                                               graph = tf.get_default_graph())\n",
        "        test_writer = tf.summary.FileWriter(\"tmp/tf/eg/validation/log\", \n",
        "                                            graph = tf.get_default_graph())\n",
        "        saver = tf.train.Saver()\n",
        "        sess.run(init)\n",
        "        for steps in range(MAX_STEPS):\n",
        "            start_time = time.time()\n",
        "            for i in tqdm(np.arange(0, Dataset.train.num_examples, BATCH_SIZE)):\n",
        "                images_feed, labels_feed = Dataset.train.next_batch(BATCH_SIZE)\n",
        "\n",
        "                feed_dict = {\n",
        "                    images_pl: np.reshape(images_feed, (-1, 28, 28, 1)),\n",
        "                    labels_pl: labels_feed\n",
        "                            }\n",
        "\n",
        "                _, loss_value = sess.run([train_op, loss], feed_dict = feed_dict)\n",
        "                duration = time.time() - start_time\n",
        "\n",
        "#                 if (steps%100 == 0):\n",
        "#                     print('Step %d: loss = %.2f (%.3f sec)' % (steps, loss_value, duration))\n",
        "#                     summary_str = sess.run(summary, feed_dict = feed_dict)\n",
        "#                     summary_writer.add_summary(summary_str, steps)\n",
        "#                     summary_writer.flush()\n",
        "\n",
        "#                 if (steps + 1) % 1000 == 0 or (steps + 1) == MAX_STEPS:\n",
        "#                     checkpoint_file = os.path.join(\"model\", \"model.ckpt\")\n",
        "#                     saver.save(sess, checkpoint_file, global_step=steps)\n",
        "#                     print('Validation Data Eval:')\n",
        "#                     log = do_eval(sess,\n",
        "#                                   eval_correct,\n",
        "#                                   images_pl,\n",
        "#                                   labels_pl,\n",
        "#                                   Dataset.validation,\n",
        "#                                   summary)\n",
        "#                     test_writer.add_summary(log, steps)\n",
        "            print('\\n Loss %d: loss = %.2f (%.3f sec)' % (steps, loss_value, duration))     \n",
        "  \n",
        "        images_feed, labels_feed = Dataset.test.images, Dataset.test.labels\n",
        "\n",
        "        feed_dict = {\n",
        "            images_pl: np.reshape(images_feed, (-1, 28, 28, 1)),\n",
        "            labels_pl: labels_feed\n",
        "                    }\n",
        "\n",
        "        accuracy = sess.run([eval_correct], feed_dict = feed_dict)\n",
        "        \n",
        "        print(\"Accuracy {}\".format(accuracy[0]/len(images_feed)))\n",
        "        \n",
        "        images_feed, labels_feed = Dataset.validation.images, Dataset.validation.labels\n",
        "\n",
        "        feed_dict = {\n",
        "            images_pl: np.reshape(images_feed, (-1, 28, 28, 1)),\n",
        "            labels_pl: labels_feed\n",
        "                    }\n",
        "        accuracy  = sess.run([eval_correct], feed_dict = feed_dict)\n",
        "        \n",
        "        print(\"Accuracy {}\".format(accuracy[0]/len(images_feed)))\n",
        "        \n",
        "        graphdef = tf.get_default_graph().as_graph_def()\n",
        "        frozen_graph = tf.graph_util.convert_variables_to_constants(sess,\n",
        "                                                                    graphdef,\n",
        "                                                                    OUTPUT_NAMES)\n",
        "        return tf.graph_util.remove_training_nodes(frozen_graph),sess, images_pl, labels_pl, eval_correct, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "PBg_gG-g0-VV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "7e9da4c9-f90a-400d-d708-ea3691df9537"
      },
      "cell_type": "code",
      "source": [
        "tf_model, sess, images_pl, labels_pl, eval_correct, logits = run_training(MNIST_DATASETS)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 96.17it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:04, 104.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 0: loss = 1.11 (4.472 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 102.41it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:03, 108.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 1: loss = 0.93 (4.201 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 102.35it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 2: loss = 0.69 (4.204 sec)\n",
            "Accuracy 0.9298\n",
            "Accuracy 0.9232\n",
            "INFO:tensorflow:Froze 12 variables.\n",
            "INFO:tensorflow:Converted 12 variables to const ops.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bPsgRXIYABWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "b118569d-6520-444a-94ff-1f705753233c"
      },
      "cell_type": "code",
      "source": [
        "images_feed, labels_feed = MNIST_DATASETS.test.images, MNIST_DATASETS.test.labels\n",
        "\n",
        "feed_dict = {\n",
        "    images_pl: np.reshape(images_feed, (-1, 28, 28, 1))\n",
        "            }\n",
        "with sess.as_default():\n",
        "  accuracy  = sess.run([logits], feed_dict = feed_dict)\n",
        "  preds = tf.argmax(accuracy[0], 1)\n",
        "  print(preds.eval())\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-d7e26ccaa9ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0maccuracy\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \"\"\"\n\u001b[0;32m--> 711\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5144\u001b[0m                        \"`eval(session=sess)`\")\n\u001b[1;32m   5145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5146\u001b[0;31m       raise ValueError(\"Cannot use the default session to evaluate tensor: \"\n\u001b[0m\u001b[1;32m   5147\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5148\u001b[0m                        \u001b[0;34m\"graph. Pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Zl6bKEbGa2Ay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "b5050277-04ce-4fa2-db3e-bbe71937fc7a"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-30c7468a6397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \"\"\"\n\u001b[0;32m--> 711\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5150\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5152\u001b[0;31m       raise ValueError(\"Cannot use the given session to evaluate tensor: \"\n\u001b[0m\u001b[1;32m   5153\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5154\u001b[0m                        \"graph.\")\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use the given session to evaluate tensor: the tensor's graph is different from the session's graph."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6DmDZXWY0-VZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "2bf267c6-1646-411b-84fb-e39c08e20840"
      },
      "cell_type": "code",
      "source": [
        "graph = tf.GraphDef()\n",
        "graph.ParseFromString(tf_model.SerializeToString())\n",
        "\n",
        "with tf.Graph().as_default() as graph:\n",
        "        # The name var will prefix every op/nodes in your graph\n",
        "        # Since we load everything in a new graph, this is not needed\n",
        "    tf.import_graph_def(graph, name=\"prefix\")\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "graph_def must be a GraphDef proto.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m       \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Parameter to MergeFrom() must be instance of same class: expected tensorflow.GraphDef got Graph.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-329087b3cacf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# The name var will prefix every op/nodes in your graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Since we load everything in a new graph, this is not needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prefix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'graph_def must be a GraphDef proto.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0minput_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: graph_def must be a GraphDef proto."
          ]
        }
      ]
    }
  ]
}