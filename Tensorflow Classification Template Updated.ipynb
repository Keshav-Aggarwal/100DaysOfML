{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow Classification Template.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "oFd0yka_0-Ul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "frMPmKbS4bZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a19dadf-84db-4d8e-e4fb-157b77155c23"
      },
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "# tf.set_random_seed(10)\n",
        "# np.random.seed(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.26.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "veAWnxz80-VQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "36f03d8c-99d4-40fe-eede-4cad2e7a7ca1"
      },
      "cell_type": "code",
      "source": [
        "MNIST_DATASETS = tf.contrib.learn.datasets.load_dataset(\"mnist\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-e0b27bad68fd>:1: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Ynn1r_c0-Us",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "STARTER_LR = 1e-4\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 10\n",
        "MAX_STEPS = 15\n",
        "IMAGE_SIZE = 28\n",
        "OUTPUT_NAMES = [\"fc2/Relu\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUeGUaMN0-Uw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def WeightsVariable(shape, name = 'weights'):\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1, name=name))\n",
        "\n",
        "def BiasVariable(shape, name = 'biases'):\n",
        "    return tf.Variable(tf.constant(0.1, shape=[shape], name='biases'))\n",
        "\n",
        "def Conv2d(x, W, B, stride = 1, padding = 'VALID', activation_fun = True):\n",
        "    filter_size = W.get_shape().as_list()\n",
        "    pad_size = filter_size[0] // 2\n",
        "    pad_mat = np.array([[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]])\n",
        "    \n",
        "    x = tf.pad(x, pad_mat)\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding = padding)\n",
        "    x = tf.nn.bias_add(x, B)\n",
        "    \n",
        "    if(activation_fun == True):\n",
        "        return tf.nn.relu(x)\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "def MaxPool2d(x, k = 2):\n",
        "    return tf.nn.max_pool(x, ksize = [1, k, k, 1], strides = [1, k, k, 1], padding = 'VALID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IC-wHEcH0-Uz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def network(images, channels = 1):\n",
        "    num_c = [8, 16, 32, 64, 64, 128]\n",
        "    #COONVOLUTION 1\n",
        "    with tf.name_scope('Conv1'):\n",
        "        weights = WeightsVariable([3, 3, channels, num_c[0]])\n",
        "        bias = BiasVariable(num_c[0])\n",
        "        conv1 = Conv2d(images, weights, bias)\n",
        "        \n",
        "    with tf.name_scope('Conv2_m'):\n",
        "        weights = WeightsVariable([3, 3, num_c[0], num_c[1]])\n",
        "        bias = BiasVariable(num_c[1])\n",
        "        conv2 = Conv2d(conv1, weights, bias)\n",
        "        pool1 = MaxPool2d(conv2)\n",
        "        \n",
        "    with tf.name_scope('Conv3'):\n",
        "        weights = WeightsVariable([3, 3, num_c[1], num_c[2]])\n",
        "        bias = BiasVariable(num_c[2])\n",
        "        conv3 = Conv2d(pool1, weights, bias)\n",
        "    \n",
        "    with tf.name_scope('Conv4_m'):\n",
        "        weights = WeightsVariable([3, 3, num_c[2], num_c[3]])\n",
        "        bias = BiasVariable(num_c[3])\n",
        "        conv4 = Conv2d(conv3, weights, bias)\n",
        "        pool2 = MaxPool2d(conv4)\n",
        "    \n",
        "    with tf.name_scope('flatten'):\n",
        "        flat = tf.layers.flatten(pool2)\n",
        "    \n",
        "    input_flat_shape = np.int32(flat.shape[1])\n",
        "    \n",
        "    with tf.name_scope('fc1'):\n",
        "        weights = WeightsVariable([input_flat_shape, 64])\n",
        "        biases = BiasVariable(64)\n",
        "        fc1 = tf.nn.relu(tf.matmul(flat, weights) + biases)\n",
        "\n",
        "    with tf.name_scope('fc2'):\n",
        "        weights = WeightsVariable([64, 10])\n",
        "        biases = BiasVariable(10)\n",
        "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases)\n",
        "    return fc2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AEWbuvcQ0-U2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_metrics(logits, labels):\n",
        "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = labels, \n",
        "                                                                   logits = logits, \n",
        "                                                                   name = 'softmax')\n",
        "    return tf.reduce_mean(cross_entropy, name = 'softmax_mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SJLUwfZX0-U6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#returns the optimizer by taking the loss\n",
        "def training(loss):\n",
        "    tf.summary.scalar('loss', loss)\n",
        "    global_step = tf.Variable(0, name = 'global_step', trainable = False)\n",
        "    \n",
        "    learning_rate = tf.train.exponential_decay(STARTER_LR, \n",
        "                                               global_step = global_step, \n",
        "                                               decay_steps = 1000, \n",
        "                                               decay_rate = 0.7, \n",
        "                                               staircase = True)\n",
        "\n",
        "    tf.summary.scalar('learning_rate', learning_rate)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "    train_op = optimizer.minimize(loss)\n",
        "    return train_op\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "57j_wcyV0-U9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluation(logits, labels):\n",
        "    correct = tf.nn.in_top_k(logits, labels, k = 1)\n",
        "#     print(logits)\n",
        "#     correct_prediction = tf.equal(tf.round(tf.nn.sigmoid(logits)), tf.cast(tf.round(labels), tf.float32))\n",
        "#     accuracy1 = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "#     all_labels_true = tf.reduce_min(tf.cast(correct_prediction, tf.float32), 1)\n",
        "#     accuracy2 = tf.reduce_mean(all_labels_true)\n",
        "#     return tf.argmax(logits[0], 1)\n",
        "    return tf.reduce_sum(tf.cast(correct, tf.int32))#, accuracy1, accuracy2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zp-v0RYc0-VB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def do_eval(sess, eval_correct, image_placeholder, labels_placeholder, data_set, summary):\n",
        "    true_count = 0\n",
        "    steps_per_epoch = data_set.num_examples // BATCH_SIZE\n",
        "    \n",
        "    num_examples = steps_per_epoch * BATCH_SIZE\n",
        "    \n",
        "    for steps in range(steps_per_epoch):\n",
        "        feed_dict = fill_feed_dict(data_set, image_placeholder, labels_placeholder)\n",
        "        log, correctness = sess.run([eval_correct], feed_dict = feed_dict)\n",
        "        true_count += correctness\n",
        "    \n",
        "    precision = float(true_count) / num_examples\n",
        "#     tf.summary.scalar('Precision', tf.constant(precision))\n",
        "    print('Num examples %d, Num Correct: %d Precisiokn @ 1: %0.04f' %\n",
        "          (num_examples, true_count, precision))\n",
        "    \n",
        "    return log    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sgIUHWfj0-VG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def placeholder_inputs(batch_size, shape = [28, 28, 1]):\n",
        "    image_placeholder = tf.placeholder(tf.float32, shape = (None, shape[0], shape[1], shape[2]))\n",
        "    label_placeholder = tf.placeholder(tf.int32, shape = (None))\n",
        "    return image_placeholder, label_placeholder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fp2vpUl80-VK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fill_feed_dict(data_set, image_placeholder, label_placeholder):\n",
        "    images_feed, labels_feed = data_set.next_batch(BATCH_SIZE)\n",
        "    feed_dict = {\n",
        "        image_placeholder: np.reshape(images_feed, (-1, 28, 28, 1)),\n",
        "        label_placeholder: labels_feed\n",
        "                }\n",
        "    return feed_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Za5Tlllw0-VN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def run_training(Dataset):\n",
        "    with tf.Graph().as_default():\n",
        "\n",
        "        images_pl, labels_pl = placeholder_inputs(BATCH_SIZE)\n",
        "        logits = network(images_pl)\n",
        "        loss = loss_metrics(logits = logits, labels = labels_pl)\n",
        "        train_op = training(loss)\n",
        "        eval_correct = evaluation(logits, labels_pl)\n",
        "        summary = tf.summary.merge_all()\n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "        \n",
        "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9) #OPTIONAL\n",
        "        sess = tf.Session(config=tf.ConfigProto(gpu_options = gpu_options))\n",
        "        \n",
        "        summary_writer = tf.summary.FileWriter(\"/tmp/tf/eg/log\", \n",
        "                                               graph = tf.get_default_graph())\n",
        "        test_writer = tf.summary.FileWriter(\"tmp/tf/eg/validation/log\", \n",
        "                                            graph = tf.get_default_graph())\n",
        "        saver = tf.train.Saver()\n",
        "        sess.run(init)\n",
        "        for steps in range(MAX_STEPS):\n",
        "            start_time = time.time()\n",
        "            for i in tqdm(np.arange(0, Dataset.train.num_examples, BATCH_SIZE)):\n",
        "                images_feed, labels_feed = Dataset.train.next_batch(BATCH_SIZE)\n",
        "\n",
        "                feed_dict = {\n",
        "                    images_pl: np.reshape(images_feed, (-1, 28, 28, 1)),\n",
        "                    labels_pl: labels_feed\n",
        "                            }\n",
        "\n",
        "                _, loss_value = sess.run([train_op, loss], feed_dict = feed_dict)\n",
        "                duration = time.time() - start_time\n",
        "\n",
        "#                 if (steps%100 == 0):\n",
        "#                     print('Step %d: loss = %.2f (%.3f sec)' % (steps, loss_value, duration))\n",
        "#                     summary_str = sess.run(summary, feed_dict = feed_dict)\n",
        "#                     summary_writer.add_summary(summary_str, steps)\n",
        "#                     summary_writer.flush()\n",
        "\n",
        "#                 if (steps + 1) % 1000 == 0 or (steps + 1) == MAX_STEPS:\n",
        "#                     checkpoint_file = os.path.join(\"model\", \"model.ckpt\")\n",
        "#                     saver.save(sess, checkpoint_file, global_step=steps)\n",
        "#                     print('Validation Data Eval:')\n",
        "#                     log = do_eval(sess,\n",
        "#                                   eval_correct,\n",
        "#                                   images_pl,\n",
        "#                                   labels_pl,\n",
        "#                                   Dataset.validation,\n",
        "#                                   summary)\n",
        "#                     test_writer.add_summary(log, steps)\n",
        "            print('\\n Loss %d: loss = %.2f (%.3f sec)' % (steps, loss_value, duration))     \n",
        "  \n",
        "        images_feed, labels_feed = Dataset.test.images, Dataset.test.labels\n",
        "\n",
        "        feed_dict = {\n",
        "            images_pl: np.reshape(images_feed, (-1, 28, 28, 1)),\n",
        "            labels_pl: labels_feed\n",
        "                    }\n",
        "\n",
        "        accuracy = sess.run([eval_correct], feed_dict = feed_dict)\n",
        "        \n",
        "        print(\"Accuracy {}\".format(accuracy[0]/len(images_feed)))\n",
        "        \n",
        "        images_feed, labels_feed = Dataset.validation.images, Dataset.validation.labels\n",
        "\n",
        "        feed_dict = {\n",
        "            images_pl: np.reshape(images_feed, (-1, 28, 28, 1)),\n",
        "            labels_pl: labels_feed\n",
        "                    }\n",
        "        accuracy  = sess.run([eval_correct], feed_dict = feed_dict)\n",
        "        \n",
        "        print(\"Accuracy {}\".format(accuracy[0]))\n",
        "        \n",
        "        graphdef = tf.get_default_graph().as_graph_def()\n",
        "        frozen_graph = tf.graph_util.convert_variables_to_constants(sess,\n",
        "                                                                    graphdef,\n",
        "                                                                    OUTPUT_NAMES)\n",
        "        return tf.graph_util.remove_training_nodes(frozen_graph),sess, images_pl, labels_pl, eval_correct, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "PBg_gG-g0-VV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3039
        },
        "outputId": "09c1caf7-4336-4d67-e50e-3021cebe886f"
      },
      "cell_type": "code",
      "source": [
        "tf_model, sess, images_pl, labels_pl, eval_correct, logits = run_training(MNIST_DATASETS)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"fc2/Relu:0\", shape=(?, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:05<00:00, 79.07it/s] \n",
            "  3%|▎         | 11/430 [00:00<00:03, 105.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 0: loss = 1.62 (5.440 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 100.08it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:04, 102.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 1: loss = 1.24 (4.301 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 101.93it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:04, 103.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 2: loss = 1.09 (4.221 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 101.72it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:03, 107.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 3: loss = 1.28 (4.231 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 102.14it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:03, 106.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 4: loss = 1.08 (4.212 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 101.81it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:03, 106.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 5: loss = 0.91 (4.226 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 102.45it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:04, 102.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 6: loss = 1.12 (4.200 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 102.17it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:03, 106.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 7: loss = 0.74 (4.211 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 102.52it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:03, 105.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 8: loss = 0.81 (4.197 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 102.06it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:04, 104.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 9: loss = 0.93 (4.217 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 102.13it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:04, 103.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 10: loss = 0.96 (4.213 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 101.47it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:03, 107.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 11: loss = 0.96 (4.240 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 102.10it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:03, 105.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 12: loss = 0.93 (4.216 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 102.01it/s]\n",
            "  3%|▎         | 11/430 [00:00<00:03, 106.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 13: loss = 0.92 (4.218 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [00:04<00:00, 101.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Loss 14: loss = 0.94 (4.239 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected dimension in the range [-1, 1), but got 1\n\t [[{{node ArgMax}} = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](strided_slice, ArgMax/dimension)]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-474771f6e745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_pl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_pl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMNIST_DATASETS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-9712c8872b12>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(Dataset)\u001b[0m\n\u001b[1;32m     60\u001b[0m                     }\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_correct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected dimension in the range [-1, 1), but got 1\n\t [[{{node ArgMax}} = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](strided_slice, ArgMax/dimension)]]\n\nCaused by op 'ArgMax', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-474771f6e745>\", line 1, in <module>\n    tf_model, sess, images_pl, labels_pl, eval_correct, logits = run_training(MNIST_DATASETS)\n  File \"<ipython-input-19-9712c8872b12>\", line 9, in run_training\n    eval_correct = evaluation(logits, labels_pl)\n  File \"<ipython-input-15-89770d032a2b>\", line 8, in evaluation\n    return tf.argmax(logits[0], 1)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 88, in argmax\n    return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 787, in arg_max\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Expected dimension in the range [-1, 1), but got 1\n\t [[{{node ArgMax}} = ArgMax[T=DT_FLOAT, Tidx=DT_INT32, output_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](strided_slice, ArgMax/dimension)]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-L_wi9yJk4dk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "374b5688-72ab-4a15-b81d-44ed877abe6d"
      },
      "cell_type": "code",
      "source": [
        "images_feed, labels_feed = MNIST_DATASETS.validation.images, MNIST_DATASETS.validation.labels\n",
        "\n",
        "feed_dict = {\n",
        "    images_pl: np.reshape(images_feed[labels_feed==3], (-1, 28, 28, 1)),labels_pl: labels_feed[labels_feed==3]\n",
        "            }\n",
        "with sess.as_default():\n",
        "  accuracy  = sess.run([eval_correct], feed_dict = feed_dict)\n",
        "  print(accuracy)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[486]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bPsgRXIYABWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2dc7c2b6-5072-482f-f5d4-b4ab82ccabde"
      },
      "cell_type": "code",
      "source": [
        "images_feed, labels_feed = MNIST_DATASETS.validation.images, MNIST_DATASETS.validation.labels\n",
        "\n",
        "feed_dict = {\n",
        "    images_pl: np.reshape(images_feed[labels_feed==3], (-1, 28, 28, 1))\n",
        "            }\n",
        "with sess.as_default():\n",
        "  accuracy  = sess.run([logits], feed_dict = feed_dict)\n",
        "  preds = np.argmax(accuracy[0], 1)\n",
        "  print(accuracy[0][0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PsjyAa5Uly6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "dab4ec30-51a4-4ac0-a48c-1622d0f7d013"
      },
      "cell_type": "code",
      "source": [
        "labels_feed[labels_feed==3]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "Zl6bKEbGa2Ay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "8d572849-11e7-48dc-f6f8-9f1cdaf4d504"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(labels_feed, preds))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.35      1.00      0.51       479\n",
            "          1       0.97      0.99      0.98       563\n",
            "          2       0.96      0.99      0.97       488\n",
            "          3       0.00      0.00      0.00       493\n",
            "          4       0.98      0.99      0.99       535\n",
            "          5       0.97      0.99      0.98       434\n",
            "          6       0.99      0.98      0.99       501\n",
            "          7       0.97      0.99      0.98       550\n",
            "          8       0.00      0.00      0.00       462\n",
            "          9       0.97      0.97      0.97       495\n",
            "\n",
            "avg / total       0.73      0.80      0.75      5000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "c5xuDiGTi2Pv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6e9eac91-0846-4b69-e9db-44d384009eaa"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(labels_feed, preds))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[478   0   1   0   0   0   0   0   0   0]\n",
            " [  2 559   0   0   0   0   0   2   0   0]\n",
            " [  1   3 482   0   0   0   0   1   0   1]\n",
            " [463   3   7   0   1   6   0   5   0   8]\n",
            " [  0   0   0   0 532   0   0   2   0   1]\n",
            " [  3   1   1   0   0 428   1   0   0   0]\n",
            " [  2   2   1   0   2   1 493   0   0   0]\n",
            " [  0   2   2   0   1   1   0 542   0   2]\n",
            " [430   4   9   0   4   6   3   2   0   4]\n",
            " [  4   2   0   0   3   0   0   6   0 480]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6DmDZXWY0-VZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "outputId": "52183c56-9ac4-4a39-ba85-fe71912ed3f0"
      },
      "cell_type": "code",
      "source": [
        "graph = tf.GraphDef()\n",
        "graph.ParseFromString(tf_model.SerializeToString())\n",
        "\n",
        "with tf.Graph().as_default() as graph:\n",
        "        # The name var will prefix every op/nodes in your graph\n",
        "        # Since we load everything in a new graph, this is not needed\n",
        "    tf.import_graph_def(graph, name=\"prefix\")\n",
        "    \n",
        "    "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36m_ProcessGraphDefParam\u001b[0;34m(graph_def, op_dict)\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Parameter to MergeFrom() must be instance of same class: expected tensorflow.GraphDef got Graph.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-329087b3cacf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# The name var will prefix every op/nodes in your graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Since we load everything in a new graph, this is not needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prefix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    383\u001b[0m   \u001b[0mop_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_def_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_registered_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m   \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ProcessGraphDefParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m   \u001b[0minput_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ProcessInputMapParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m   \u001b[0mreturn_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ProcessReturnElementsParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36m_ProcessGraphDefParam\u001b[0;34m(graph_def, op_dict)\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'graph_def must be a GraphDef proto.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# If we're using the graph_def provided by the caller, modify graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: graph_def must be a GraphDef proto."
          ]
        }
      ]
    }
  ]
}